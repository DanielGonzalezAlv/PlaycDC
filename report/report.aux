\relax 
\citation{DBLP:journals/corr/abs-1804-02767}
\@writefile{toc}{\contentsline {section}{\numberline {1}Abstract [Daniel \& Frank]}{1}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Stock photos of an envisioned camera-equipped contact lense (left) and a mysterious black-jack player (right)\relax }}{1}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Introduction [Daniel \& Frank]}{1}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Dataset [Daniel]}{2}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Semi automatic detection of the convex hulls.\relax }}{2}}
\newlabel{fig-data-prep}{{2}{2}}
\citation{cimpoi14describing}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Some cards pasted into canvases of 3000x3000 pixels.\relax }}{3}}
\newlabel{fig-canvas}{{3}{3}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Images of the previous paragraph after randomly performed transformations.\relax }}{4}}
\newlabel{img-cards examples}{{4}{4}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces The green box represents the ``worst case'' BB after applying some rotations, while the red box represents the BB of the convex hull.\relax }}{4}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{bb-ch}{{5}{4}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {\relax \fontsize {7}{8}\selectfont Queen of spades with a resolution of 600x900 pixels.}}}{4}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {\relax \fontsize {7}{8}\selectfont Detected convex hulls. The red points were saved as a NumPy array.}}}{4}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces { }}}{4}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {}}}{4}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(d)}{\ignorespaces {}}}{4}}
\citation{DBLP:journals/corr/RenHG015}
\citation{DBLP:journals/corr/RedmonDGF15}
\citation{DBLP:journals/corr/RedmonF16}
\citation{DBLP:journals/corr/abs-1804-02767}
\citation{DBLP:journals/corr/LiuAESR15}
\citation{DBLP:journals/corr/abs-1708-02002}
\citation{DBLP:journals/corr/LinDGHHB16}
\@writefile{toc}{\contentsline {section}{\numberline {4}Related work - The object detection landscape [Frank]}{5}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces The Faster R-CNN architecture. Note the weight sharing between RPN Network and the classifier.\relax }}{5}}
\newlabel{fig:rcnn-architectures}{{6}{5}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Evaluation of different object detection algorithms on the COCO dataset from the RetinaNet paper. Currently, evaluation performance is dominated by the RetinaNet architecture.\relax }}{5}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Methods [Frank]}{5}}
\newlabel{SC@1}{{\caption@xref {??}{ on input line 215}}{6}}
\newlabel{fig:yolomodel}{{5}{6}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces For each of the $S^2$ grid cells, we make one objectness prediction and four positional predictions for bounding boxes relative to each of the $B$ anchor boxes as well as a global class prediction vector. Combination of these values using NMS grants the final predictions.\relax }}{6}}
\newlabel{SC@2}{{\caption@xref {??}{ on input line 225}}{6}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces For each grid cell, we predict vectors of multiple bounding boxes (here, only one bounding box is predicted, $B=1$), but only one class prediction vector of length $C$ (here, $C$=3).\relax }}{6}}
\newlabel{fig:cell}{{9}{6}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces A simple YOLO-v3 feature extractor with a grid size of $7\times 7$, $B=2$ and 20 classes, making the final prediction vector ($7 \times 7 \times 30$) - dimensional. \relax }}{7}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Webcam deployment [Frank \& Daniel]}{8}}
\@writefile{toc}{\contentsline {section}{\numberline {7}Evaluation [Frank \& Daniel]}{8}}
\@writefile{toc}{\contentsline {section}{\numberline {8}Results [Frank \& Daniel]}{8}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Precision and recall values have been calculated using a IOU threshold of 0.5. mAP values are based on averaged precision values over IOU thresholds of $[0.1, 0.2, \dots  0.8, 0.9] $ \relax }}{8}}
\newlabel{tab:res}{{1}{8}}
\citation{Everingham15}
\newlabel{SC@3}{{\caption@xref {??}{ on input line 331}}{9}}
\newlabel{fig:curve}{{8}{9}}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces Precision values (for a IOU threshold of 0.5) after each training epoch evaluated on the test set pertaining to the dataset being trained on. As you would expect, the easier the dataset, the faster the training and the better the results. \relax }}{9}}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces Successful cases of detection of images that are pretty representative of the training distribution\relax }}{9}}
\newlabel{fig:testcases}{{12}{9}}
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces Successful cases of detection (top row) and typical fail cases (bottom row) along with corresponding classification scores.\relax }}{10}}
\newlabel{fig:testcases}{{13}{10}}
\@writefile{toc}{\contentsline {section}{\numberline {9}Discussion and Future Work [Frank \& Daniel]}{10}}
\@writefile{toc}{\contentsline {section}{\numberline {10}Conclusion [Frank \& Daniel]}{12}}
\bibstyle{acm}
\bibdata{cite}
\bibcite{cimpoi14describing}{1}
\bibcite{Everingham15}{2}
\bibcite{DBLP:journals/corr/LinDGHHB16}{3}
\bibcite{DBLP:journals/corr/abs-1708-02002}{4}
\bibcite{DBLP:journals/corr/LiuAESR15}{5}
\bibcite{DBLP:journals/corr/RedmonDGF15}{6}
\bibcite{DBLP:journals/corr/RedmonF16}{7}
\bibcite{DBLP:journals/corr/abs-1804-02767}{8}
\bibcite{DBLP:journals/corr/RenHG015}{9}
